{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambio_enti(s):\n",
    "    llista = s.lower().split(' ')\n",
    "    n_llista = [i.capitalize() if i != 'de' and i != 'del' else i for i in llista ]\n",
    "    s = ' '.join(n_llista)\n",
    "    if '-' in s:\n",
    "        llista = s.lower().split('-')\n",
    "        n_llista = [i.capitalize() if i != 'de' and i != 'del' else i for i in llista ]\n",
    "        s = ' '.join(n_llista)\n",
    "    if '(la)' in s:\n",
    "        s = 'La '+s.replace('(la)','').strip()\n",
    "    if '(el)' in s:\n",
    "        s = 'El '+s.replace('(el)','').strip()\n",
    "    if '(los)' in s:\n",
    "        s = 'Los '+s.replace('(los)','').strip()\n",
    "    if '(las)' in s:\n",
    "        s = 'Las '+s.replace('(las)','').strip()   \n",
    "\n",
    "    if 'de La ' in s:\n",
    "        s = s.replace('de La ', 'de la ')\n",
    "    if 'de Las ' in s:\n",
    "        s = s.replace('de Las ', 'de las ')\n",
    "    if 'de Los ' in s:\n",
    "        s = s.replace('de Los ', 'de los ')\n",
    "    if ' Y ' in s:\n",
    "        s = s.replace(' Y ', ' y ')\n",
    "    if ' O ' in s:\n",
    "        s = s.replace(' O ', ' o ')\n",
    "    return s\n",
    "def scrap_wiki_pop(x):\n",
    "    provincia, municipio, entidad = x\n",
    "    n_e = cambio_enti(entidad)\n",
    "    try:\n",
    "        URL = f\"https://es.wikipedia.org/wiki/{n_e.replace(' ','_')}\"\n",
    "        #print(URL)\n",
    "        r = requests.get(URL)\n",
    "        soup = BeautifulSoup(r.content, 'lxml') \n",
    "        bodies = soup.findAll('tbody')\n",
    "        a = False\n",
    "        for e in bodies:\n",
    "            if 'Población' in e.text:\n",
    "                r_tabla = e.findAll('tr')\n",
    "                a=True\n",
    "                break\n",
    "\n",
    "        if not a: \n",
    "            URL = f\"https://es.wikipedia.org/wiki/{n_e.replace(' ','-')}_({municipio.replace(' ','_')})\"\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.content, 'lxml') \n",
    "            bodies = soup.findAll('tbody')\n",
    "            for e in bodies:\n",
    "                if 'Población' in e.text: a=True\n",
    "        \n",
    "        if not a: \n",
    "            URL = f\"https://es.wikipedia.org/wiki/{n_e.replace(' ','_')}_({municipio.replace(' ','_')})\"\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.content, 'lxml') \n",
    "            bodies = soup.findAll('tbody')\n",
    "            for e in bodies:\n",
    "                if 'Población' in e.text:\n",
    "                    a=True\n",
    "        if not a: \n",
    "            URL = f\"https://es.wikipedia.org/wiki/{n_e.replace(' ','_')}_({provincia})\"\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.content, 'lxml') \n",
    "            bodies = soup.findAll('tbody')\n",
    "            for e in bodies:\n",
    "                if 'Población' in e.text:\n",
    "                    a=True\n",
    "        table = soup.find('table', {'class': 'infobox geography vcard'})\n",
    "        rows = table.findAll('tr')\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            if len(cols) > 0 and 'hab' in cols[0].text and 'km' not in cols[0].text:\n",
    "                pob = unicodedata.normalize(\"NFKD\", str(cols[0].get_text(strip=True)))\n",
    "                \n",
    "        return pob\n",
    "    except:\n",
    "        # print(n_e)\n",
    "        # i += 1\n",
    "        # if i > 1000:\n",
    "        #     i = 0\n",
    "        #     print('ok')\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16 682 hab.(2021)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrap_wiki_pop(['Girona','Berga','Berga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/castilla_leon/main.csv', index_col=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['population'] = df.apply(lambda x: scrap_wiki_pop((x['Provincia'],x['Municipio'],x['Entidad Singular de Población'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d00a1931e609772884d4e4263bd83cd526fe20ad88d0f2499e0ed4fd2f889d0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ironhack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
